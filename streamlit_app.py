import warnings
warnings.filterwarnings("ignore", message=".*missing ScriptRunContext.*")

import streamlit as st
from collections import defaultdict
from search_engine import SemanticSearchEngine

# æ¨¡å‹å¿«å–
@st.cache_resource
def get_engine():
    return SemanticSearchEngine('data/scam_dataset_tw_54500.csv', risk_threshold=0.7)

# è®€å–æ•…äº‹åº«
@st.cache_data(ttl=600)
def load_stories():
    df = pd.read_csv('data/scam_story.csv', dtype=str)
    df.fillna('', inplace=True)
    return df

engine = get_engine()
stories_df = load_stories()

# å¿«å–æŸ¥è©¢çµæœï¼Œé¿å…åè¦†è¨ˆç®—ç›¸åŒ query
@st.cache_data(ttl=600)
def cached_search(query: str, top_k: int = 10):
    return engine.search(query, top_k=top_k)

@st.cache_data(ttl=600)
def cached_analysis(query: str):
    return engine.sentence_analysis(query)

@st.cache_data(ttl=600)
def cached_highlight(query: str):
    return engine.highlight_keywords(query)

# é é¢è¨­å®š
st.set_page_config(
    page_title="AskSense è©é¨™æª¢æ¸¬å·¥å…·",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Sidebar: Developer Info & Expertise
with st.sidebar:
    st.header("å°ˆæ¡ˆç°¡æ˜“èªªæ˜")
    st.markdown(
        """
        é€™å€‹å·¥å…·è®“ä½ å¯ä»¥è²¼ä¸Šä»»æ„æ–‡å­—ï¼Œå¿«é€Ÿæ¨™ç¤ºå‡ºå¯èƒ½æœ‰é¢¨éšªæˆ–å•é¡Œçš„å¥å­ã€‚  
        å®ƒæœƒè¨˜æ†¶ä½ æœ€è¿‘çš„æª¢æŸ¥çµæœï¼Œé‡è¤‡æª¢æŸ¥ç›¸åŒæ–‡å­—ï¼Œé€Ÿåº¦æœƒæ›´å¿«ã€‚  
        éå¸¸é©åˆæƒ³è¦è¼•é¬†æ‰¾å‡ºè‡ªå·±æ–‡å­—ä¸­æ½›åœ¨å®‰å…¨æˆ–å…§å®¹é¢¨éšªçš„ä»»ä½•äººï¼
        """
    )
    st.markdown("---")
    st.header("é–‹ç™¼è€…è³‡è¨Š")
    st.markdown(
        """
        **èƒŒæ™¯**
        - åœ‹ç«‹é™½æ˜äº¤é€šå¤§å­¸ è³‡è¨Šå·¥ç¨‹å­¸ç³»
        - AI èˆ‡è³‡å®‰ç ”ç©¶

        **å°ˆé•·**
        - è‡ªç„¶èªè¨€è™•ç† (NLP)
        - æ·±åº¦å­¸ç¿’æ¨¡å‹é–‹ç™¼ (SBERT)
        - ç³»çµ±è³‡å®‰èˆ‡é˜²è­·ç­–ç•¥
        """
    )
    st.markdown("Â© 2025 JN AskSense. All rights reserved.")

# ä¸»å€åŸŸï¼šæ¨™é¡Œèˆ‡è¼¸å…¥
st.title('ğŸ” AskSense è©é¨™æ–‡å­—æª¢æ¸¬ & èªæ„æœå°‹å·¥å…·')
st.markdown('å°ˆç‚ºå°ç£ç”¨æˆ¶è¨­è¨ˆï¼Œå¿«é€Ÿä¸”å®‰å…¨åœ°æªå‡ºæ½›åœ¨è©é¨™è¨Šæ¯ã€‚')

# é™åˆ¶æœ€å¤§è¼¸å…¥é•·åº¦ä»¥é˜²éåº¦é‹ç®—
query = st.text_area('ğŸ“¥ è²¼ä¸Šè¦æª¢æ¸¬çš„æ–‡å­— (æœ€å¤š1000å­—)ï¼š', height=200, max_chars=1000)

if st.button('é–‹å§‹åˆ†æ') and query:
    with st.spinner('åˆ†æä¸­ï¼Œè«‹ç¨å€™â€¦'):
        # å¿«å–æŸ¥è©¢èˆ‡é«˜éšè¨ˆç®—
        top10 = cached_search(query)
        analysis = cached_analysis(query)
        hits, highlighted = cached_highlight(query)
        # å½™æ•´ Top3 è©é¨™é¡å‹
        type_scores = defaultdict(list)
        for r in top10:
            type_scores[r['type']].append(r['score'])
        top_types = sorted(
            ((t, max(scores)) for t, scores in type_scores.items()),
            key=lambda x: x[1], reverse=True
        )[:3]

    # é¡¯ç¤ºæœ€å¯èƒ½ Top3 é¡å‹
    st.subheader('ğŸš© æœ€å¯èƒ½çš„ 3 ç¨®è©é¨™é¡å‹')
    for t, sc in top_types:
        st.markdown(f"- **{t}** (æœ€é«˜ç›¸ä¼¼åº¦: {sc:.4f})")

    # é¡¯ç¤ºé—œéµè©å’Œæ¨™ç¤º
    if hits:
        col1, col2 = st.columns(2)
        with col1:
            st.subheader('ğŸ”‘ å‘½ä¸­é—œéµè©')
            st.write(', '.join(hits))
        with col2:
            st.subheader('ğŸ– é—œéµè©æ¨™ç¤º')
            st.write(highlighted)

    # é€å¥é¢¨éšªåˆ†æ (ç¶ /é»ƒ/ç´…)
    st.subheader('ğŸ” é€å¥é¢¨éšªåˆ†æ')
    for item in analysis:
        icon = {'ç¶ ':'ğŸŸ¢','é»ƒ':'ğŸŸ¡','ç´…':'ğŸ”´'}[item['level']]
        msg = f"{icon} [{item['level']}] {item['sentence']} (æ¯”å°ç‡: {item['score']:.4f})"
        if item['level'] == 'ç´…':
            st.error(msg)
        elif item['level'] == 'é»ƒ':
            st.warning(msg)
        else:
            st.success(msg)

    # é¡ä¼¼è©é¨™æ¡ˆä¾‹
    st.subheader('ğŸ’¡ é¡ä¼¼è©é¨™æ¡ˆä¾‹')
    main_type = top_types[0][0] if top_types else None
    if main_type:
        st.markdown(f"**{main_type}** æ¡ˆä¾‹ç¤ºç¯„ï¼š")
        examples = stories_df[stories_df['type'] == main_type]['Content'].tolist()
        for ex in examples[:3]:  # å‰3å€‹æ¡ˆä¾‹
            st.write(f"- {ex}")

    st.markdown('---')
    st.markdown('æ›´å¤šè³‡æºï¼š [165 é˜²è©é”äºº](https://165.npa.gov.tw)')